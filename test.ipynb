{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
      "Starting program on device [gpu(id=0)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from 'polaron_data' with 28928 frames of 384 atoms.\n",
      "System initialized with 171426 parameters.\n",
      "Iter  0 : Loss = 6.38994  Loss_E = 0.10773  Loss_F = 0.20185  Time = 2.71\n",
      "Iter  1000 : Loss = 4.56492  Loss_E = 0.06633  Loss_F = 0.14424  Time = 5.38\n",
      "Iter  2000 : Loss = 3.76446  Loss_E = 0.10843  Loss_F = 0.11866  Time = 2.88\n",
      "Iter  3000 : Loss = 3.45661  Loss_E = 0.10709  Loss_F = 0.10890  Time = 2.88\n",
      "Iter  4000 : Loss = 3.21452  Loss_E = 0.03912  Loss_F = 0.10174  Time = 2.88\n",
      "Iter  5000 : Loss = 2.87790  Loss_E = 0.03391  Loss_F = 0.09296  Time = 2.88\n",
      "Iter  6000 : Loss = 2.59096  Loss_E = 0.02107  Loss_F = 0.08377  Time = 2.88\n",
      "Iter  7000 : Loss = 2.62747  Loss_E = 0.03716  Loss_F = 0.08480  Time = 2.88\n",
      "Iter  8000 : Loss = 2.66845  Loss_E = 0.02868  Loss_F = 0.08631  Time = 2.88\n",
      "Iter  9000 : Loss = 2.25745  Loss_E = 0.01204  Loss_F = 0.07469  Time = 2.88\n",
      "Iter  10000 : Loss = 2.37409  Loss_E = 0.02002  Loss_F = 0.07848  Time = 2.88\n",
      "Iter  11000 : Loss = 2.23793  Loss_E = 0.02104  Loss_F = 0.07395  Time = 2.88\n",
      "Iter  12000 : Loss = 2.04086  Loss_E = 0.01605  Loss_F = 0.06754  Time = 2.88\n",
      "Iter  13000 : Loss = 1.98197  Loss_E = 0.01974  Loss_F = 0.06693  Time = 2.88\n",
      "Iter  14000 : Loss = 2.32833  Loss_E = 0.01733  Loss_F = 0.07873  Time = 2.88\n",
      "Iter  15000 : Loss = 2.07358  Loss_E = 0.01794  Loss_F = 0.07007  Time = 2.88\n",
      "Iter  16000 : Loss = 1.92889  Loss_E = 0.01649  Loss_F = 0.06526  Time = 2.88\n",
      "Iter  17000 : Loss = 1.99826  Loss_E = 0.02327  Loss_F = 0.06887  Time = 2.88\n",
      "Iter  18000 : Loss = 1.85065  Loss_E = 0.01289  Loss_F = 0.06400  Time = 2.88\n",
      "Iter  19000 : Loss = 1.96858  Loss_E = 0.02170  Loss_F = 0.06788  Time = 2.88\n",
      "Iter  20000 : Loss = 1.79351  Loss_E = 0.01321  Loss_F = 0.06209  Time = 2.88\n",
      "Iter  21000 : Loss = 1.67159  Loss_E = 0.01350  Loss_F = 0.05905  Time = 2.88\n",
      "Iter  22000 : Loss = 1.70844  Loss_E = 0.01032  Loss_F = 0.06044  Time = 2.88\n",
      "Iter  23000 : Loss = 1.68222  Loss_E = 0.01166  Loss_F = 0.05948  Time = 2.88\n",
      "Iter  24000 : Loss = 1.62805  Loss_E = 0.00726  Loss_F = 0.05770  Time = 2.88\n",
      "Iter  25000 : Loss = 1.56223  Loss_E = 0.01030  Loss_F = 0.05648  Time = 2.88\n",
      "Iter  26000 : Loss = 1.59513  Loss_E = 0.00853  Loss_F = 0.05771  Time = 2.88\n",
      "Iter  27000 : Loss = 1.64041  Loss_E = 0.00770  Loss_F = 0.05937  Time = 2.88\n",
      "Iter  28000 : Loss = 1.60085  Loss_E = 0.00656  Loss_F = 0.05802  Time = 2.88\n",
      "Iter  29000 : Loss = 1.48523  Loss_E = 0.00763  Loss_F = 0.05494  Time = 3.00\n",
      "Iter  30000 : Loss = 1.48934  Loss_E = 0.00901  Loss_F = 0.05506  Time = 2.88\n",
      "Iter  31000 : Loss = 1.65046  Loss_E = 0.01173  Loss_F = 0.06096  Time = 2.88\n",
      "Iter  32000 : Loss = 1.51833  Loss_E = 0.00562  Loss_F = 0.05626  Time = 2.88\n",
      "Iter  33000 : Loss = 1.45085  Loss_E = 0.01223  Loss_F = 0.05469  Time = 2.88\n",
      "Iter  34000 : Loss = 1.46035  Loss_E = 0.01164  Loss_F = 0.05508  Time = 2.88\n",
      "Iter  35000 : Loss = 1.50814  Loss_E = 0.00894  Loss_F = 0.05699  Time = 2.88\n",
      "Iter  36000 : Loss = 1.47076  Loss_E = 0.01012  Loss_F = 0.05563  Time = 2.88\n",
      "Iter  37000 : Loss = 1.41182  Loss_E = 0.00692  Loss_F = 0.05456  Time = 2.88\n",
      "Iter  38000 : Loss = 1.35883  Loss_E = 0.00621  Loss_F = 0.05252  Time = 2.88\n",
      "Iter  39000 : Loss = 1.38025  Loss_E = 0.00802  Loss_F = 0.05330  Time = 2.88\n",
      "Iter  40000 : Loss = 1.34358  Loss_E = 0.00601  Loss_F = 0.05201  Time = 2.88\n",
      "Iter  41000 : Loss = 1.28714  Loss_E = 0.00580  Loss_F = 0.05084  Time = 2.88\n",
      "Iter  42000 : Loss = 1.27726  Loss_E = 0.00473  Loss_F = 0.05048  Time = 2.88\n",
      "Iter  43000 : Loss = 1.38034  Loss_E = 0.00704  Loss_F = 0.05450  Time = 2.88\n",
      "Iter  44000 : Loss = 1.33919  Loss_E = 0.01032  Loss_F = 0.05280  Time = 2.88\n",
      "Iter  45000 : Loss = 1.21958  Loss_E = 0.00693  Loss_F = 0.04917  Time = 2.88\n",
      "Iter  46000 : Loss = 1.32253  Loss_E = 0.01301  Loss_F = 0.05302  Time = 2.88\n",
      "Iter  47000 : Loss = 1.26837  Loss_E = 0.00723  Loss_F = 0.05113  Time = 2.88\n",
      "Iter  48000 : Loss = 1.21581  Loss_E = 0.00729  Loss_F = 0.04904  Time = 2.88\n",
      "Iter  49000 : Loss = 1.22685  Loss_E = 0.00961  Loss_F = 0.05039  Time = 2.88\n",
      "Iter  50000 : Loss = 1.24063  Loss_E = 0.00875  Loss_F = 0.05101  Time = 2.88\n",
      "Iter  51000 : Loss = 1.18357  Loss_E = 0.00659  Loss_F = 0.04874  Time = 2.88\n",
      "Iter  52000 : Loss = 1.24907  Loss_E = 0.00939  Loss_F = 0.05136  Time = 2.88\n",
      "Iter  53000 : Loss = 1.12043  Loss_E = 0.00585  Loss_F = 0.04714  Time = 2.88\n",
      "Iter  54000 : Loss = 1.11314  Loss_E = 0.00566  Loss_F = 0.04684  Time = 2.88\n",
      "Iter  55000 : Loss = 1.14019  Loss_E = 0.00817  Loss_F = 0.04786  Time = 2.88\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/global/homes/r/ruiqig/deepmd-jax/test.ipynb Cell 1\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bperl/global/homes/r/ruiqig/deepmd-jax/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39mfor\u001b[39;00m iteration \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(total_steps):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bperl/global/homes/r/ruiqig/deepmd-jax/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m     batch, lattice_args \u001b[39m=\u001b[39m train_data\u001b[39m.\u001b[39mget_batch(batch_size)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bperl/global/homes/r/ruiqig/deepmd-jax/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m     variables, opt_state, state_args \u001b[39m=\u001b[39m train_step(batch, variables, opt_state, state_args, static_args)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bperl/global/homes/r/ruiqig/deepmd-jax/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m     \u001b[39mif\u001b[39;00m iteration \u001b[39m%\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bperl/global/homes/r/ruiqig/deepmd-jax/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m         beta \u001b[39m=\u001b[39m l_smoothing \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m/\u001b[39ml_smoothing)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(iteration\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(_cls, count, mu, nu)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import jit, grad, random, tree_util\n",
    "import jax, optax\n",
    "import flax.linen as nn\n",
    "from deepmd_jax.data import DataSystem\n",
    "from deepmd_jax.model import DPModel\n",
    "import pickle\n",
    "from time import time\n",
    "# jax.config.update('jax_enable_x64', True)\n",
    "jax.config.update('jax_default_matmul_precision', 'float32')\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "print('Starting program on device', jax.devices())\n",
    "\n",
    "save_name    = 'model_polaron.pkl'\n",
    "train_data   = DataSystem('polaron_data', ['coord', 'box', 'force', 'energy'])\n",
    "orthorombic  = True\n",
    "rcut         = 7.6\n",
    "embed_widths = [16, 32, 32]\n",
    "fit_widths   = [128, 128]\n",
    "axis_neuron  = 16\n",
    "batch_size   = 1\n",
    "lr           = 0.002 \n",
    "beta2        = 0.99\n",
    "s_pref_e     = 0.02\n",
    "l_pref_e     = 1\n",
    "s_pref_f     = 1000\n",
    "l_pref_f     = 100\n",
    "total_steps  = 400000\n",
    "decay_steps  = 4000\n",
    "decay_rate   = 0.95\n",
    "\n",
    "RANDOM_SEED  = np.random.randint(1000)\n",
    "l_smoothing  = 20\n",
    "getstat_bs   = 64\n",
    "\n",
    "train_data.compute_lattice_candidate(rcut)\n",
    "model = DPModel({'embed_widths':embed_widths,\n",
    "                 'fit_widths':fit_widths,\n",
    "                 'axis_neuron':axis_neuron,\n",
    "                 'Ebias':train_data.compute_Ebias()})\n",
    "batch, lattice_args = train_data.get_batch(getstat_bs)\n",
    "static_args = nn.FrozenDict({'lattice': lattice_args | {'ortho':orthorombic},\n",
    "                            'rcut':rcut,\n",
    "                            'type_index':tuple(train_data.type_index),\n",
    "                            'ntype_index':tuple(lattice_args['lattice_max']*train_data.type_index)})\n",
    "model.get_stats(batch['coord'], batch['box'], static_args)\n",
    "variables = model.init(random.PRNGKey(RANDOM_SEED), batch['coord'][0], batch['box'][0], static_args)\n",
    "lr_scheduler = optax.exponential_decay(init_value=lr, transition_steps=decay_steps,\n",
    "                    decay_rate=decay_rate, transition_begin=0, staircase=True)\n",
    "optimizer = optax.adam(learning_rate=lr_scheduler, b2=beta2)\n",
    "opt_state = optimizer.init(variables)\n",
    "loss, loss_and_grad = model.get_loss_ef_fn()\n",
    "print('System initialized with', sum(i.size for i in tree_util.tree_flatten(variables)[0]), 'parameters.')\n",
    "\n",
    "model.params['normalizer'] *= 1.\n",
    "model.params['e3norm'] = 1.\n",
    "\n",
    "def train_step(batch, variables, opt_state, state_args, static_args):\n",
    "    r = lr_scheduler(state_args['iteration']) / lr\n",
    "    pref = {'e': s_pref_e*r + l_pref_e*(1-r), 'f': s_pref_f*r + l_pref_f*(1-r)}\n",
    "    (loss_total, (loss_e, loss_f)), grads = loss_and_grad(variables, batch, pref, static_args)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    variables = optax.apply_updates(variables, updates)\n",
    "    state_args['loss_avg'] = state_args['loss_avg'] * (1-1/l_smoothing) + loss_total\n",
    "    state_args['le_avg'] = state_args['le_avg'] * (1-1/l_smoothing) + loss_e\n",
    "    state_args['lf_avg'] = state_args['lf_avg'] * (1-1/l_smoothing) + loss_f\n",
    "    state_args['iteration'] += 1\n",
    "    return variables, opt_state, state_args\n",
    "train_step = jit(train_step, static_argnums=(4,))\n",
    "\n",
    "state_args = {'le_avg':0., 'lf_avg':0., 'loss_avg':0., 'iteration':0}\n",
    "tic = time()\n",
    "# for iteration in range(0):\n",
    "for iteration in range(total_steps):\n",
    "    batch, lattice_args = train_data.get_batch(batch_size)\n",
    "    variables, opt_state, state_args = train_step(batch, variables, opt_state, state_args, static_args)\n",
    "    if iteration % 1000 == 0:\n",
    "        beta = l_smoothing * (1 - (1/l_smoothing)**(iteration+1))\n",
    "        print('Iter ', iteration,\n",
    "              ': Loss = %.5f' % (state_args['loss_avg']/beta)**0.5,\n",
    "              ' Loss_E = %.5f' % ((state_args['le_avg']/beta)**0.5/train_data.natoms),\n",
    "              ' Loss_F = %.5f' % (state_args['lf_avg']/beta)**0.5,\n",
    "              ' Time = %.2f' % (time()-tic))\n",
    "        tic = time()\n",
    "\n",
    "with open(save_name, 'wb') as file:\n",
    "    pickle.dump({'model':model, 'variables':variables}, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "e, debug = model.apply(variables, train_data.data['coord'][i], train_data.data['box'][i], static_args)\n",
    "(r_NM, embed_nmC, R_4NM, G_N4C, Feat_NX, fit_n1) = debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(0.5569, dtype=float32),\n",
       " Array(7.99721, dtype=float32),\n",
       " Array(0.37572, dtype=float32))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(G_N4C[:,0]) , np.std(G_N4C[:,0]) / np.std(G_N4C[:,1:]), np.std(Feat_NX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(0.42991, dtype=float32),\n",
       " Array(7.5261, dtype=float32),\n",
       " Array(0.18197, dtype=float32))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(G_N4C[:,0]) , np.std(G_N4C[:,0]) / np.std(G_N4C[:,1:]), np.std(Feat_NX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(0.32271, dtype=float32),\n",
       " Array(7.7443, dtype=float32),\n",
       " Array(0.09806, dtype=float32))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(G_N4C[:,0]) , np.std(G_N4C[:,0]) / np.std(G_N4C[:,1:]), np.std(Feat_NX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(0.43412, dtype=float32),\n",
       " Array(7.55578, dtype=float32),\n",
       " Array(0.18473, dtype=float32))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(G_N4C[:,0]) , np.std(G_N4C[:,0]) / np.std(G_N4C[:,1:]), np.std(Feat_NX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(0.4334, dtype=float32),\n",
       " Array(8.00204, dtype=float32),\n",
       " Array(0.19608, dtype=float32))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(G_N4C[:,0]) , np.std(G_N4C[:,0]) / np.std(G_N4C[:,1:]), np.std(Feat_NX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(0.31626, dtype=float32),\n",
       " Array(7.32899, dtype=float32),\n",
       " Array(0.10737, dtype=float32))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(G_N4C[:,0]) , np.std(G_N4C[:,0]) / np.std(G_N4C[:,1:]), np.std(Feat_NX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(45.24406, dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.params['normalizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Array(0.15338, dtype=float32), Array(0.13168, dtype=float32)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.params['srstd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
